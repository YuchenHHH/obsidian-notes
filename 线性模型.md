  
## Fréchet 导数定义
设有一个从 $\mathbb{R}^n$ 到 $\mathbb{R}$的函数 $f: \mathbb{R}^n \to \mathbb{R}$，如果 $f$ 在点 $\mathbf{x}$ 处 Fréchet 可微，则存在一个线性算子 $L: \mathbb{R}^n \to \mathbb{R}$，满足
$$
f(\mathbf{x}+\mathbf{h}) = f(\mathbf{x}) + L(\mathbf{h}) + o(\|\mathbf{h}\|),
$$
其中 $L(\mathbf{h})$ 通常可以写为内积
$$
L(\mathbf{h}) = \nabla f(\mathbf{x})^T \mathbf{h}.
$$
更进一步
$$
f(\mathbf{W} + \mathbf{H}) - f(\mathbf{W}) \approx \langle \nabla_{\mathbf{W}} f, \mathbf{H} \rangle.
$$
其中内积取 $\langle A, B \rangle = \text{tr}(A^T B)$ 是 Frobenius 内积。

## 随机梯度下降（SGD）和 Mini-batch 的区别

| **方法**                                | **每次计算的梯度样本数**            | **计算效率**      | **收敛速度**        | **噪声影响**    |
| ------------------------------------- | ------------------------- | ------------- | --------------- | ----------- |
| **Batch Gradient Descent**            | 所有训练数据                    | 慢（每次迭代计算所有样本） | 可能更稳定，但计算量大     | 低（使用所有数据梯度） |
| **Mini-batch Gradient Descent**       | m 个样本（通常 16, 32, 64, 128） | 适中（利用并行计算）    | 适中（平衡稳定性和计算效率）  | 适中          |
| **Stochastic Gradient Descent (SGD)** | 1 个样本                     | 快（每次计算一个样本）   | 震荡较大，但可能逃离局部极小值 | 高（梯度波动大）    |

### SGD 是否可以看作 batch size = 1 的 mini-batch

是的，**SGD 可以被认为是 mini-batch 梯度下降的一种特殊情况，其中 mini-batch 大小** m = 1。

• **相似之处**：

• 计算梯度的方式相同，都是通过随机选取数据子集（mini-batch or 单个样本）。

• SGD 只是 mini-batch 下降的极端情况，即 batch size 为 1。

• **不同之处**：

• mini-batch 梯度下降使用多个样本计算梯度，能减少梯度的方差，提高稳定性，而 SGD 由于只用 1 个样本计算梯度，更新过程震荡较大。


**何时使用 SGD vs Mini-batch？**

• **SGD（batch size = 1）**

• 适用于 **大规模数据集**，因为它每次迭代计算的开销较小。
• 适用于 **在线学习** 场景，例如强化学习或流式数据训练。
• 适用于 **容易陷入局部最优** 的情况，因为它的随机性有助于跳出局部极小值。

• **Mini-batch（batch size > 1）**

• 适用于 **深度学习**，因为它能充分利用 GPU 进行并行计算，提高效率。
• 适用于 **收敛更稳定** 的场景，避免 SGD 震荡太大影响收敛速度。

交叉熵（Cross-Entropy）是一种常用于**分类问题**的损失函数，尤其在逻辑回归（Logistic Regression）、Softmax 分类器和神经网络中。它用于衡量模型预测的概率分布与真实分布之间的差距。

---

## **交叉熵的定义**

交叉熵衡量两个概率分布 $P$ 和 $Q$ 之间的相似性，定义如下：
$H(P, Q) = - \sum_{i} P(i) \log Q(i)$
其中：
• $P(i)$ 是真实分布（ground truth label）。
• $Q(i)$ 是模型的预测分布。  

如果真实分布是**one-hot** 编码（即真实类别的概率为 1，其余类别的概率为 0），则交叉熵可以简化为：
$H(P, Q) = -\log Q(y)$
其中 y 是真实类别的索引，Q(y) 是模型预测该类别的概率。

## **分类问题的 Ground Truth（真实标签）**

在机器学习和深度学习中的**分类问题**，**Ground Truth**（简称 GT）指的是 **数据的真实类别标签**，即在训练或测试时，每个样本实际所属的正确类别。

---

**1. Ground Truth 的定义**

在分类问题中，给定一个数据集 $\mathcal{D}$，每个样本由特征 $\mathbf{x}$ 和真实类别标签 $y$ 组成：
$\mathcal{D} = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dots, (\mathbf{x}_N, y_N)\}$
其中：
• $\mathbf{x}_i$ 是第 $i$ 个样本的特征（如图像、文本、数值向量）。
• $y_i$ 是该样本的 **Ground Truth**（真实类别标签）。
• 目标是训练模型 $f(\mathbf{x})$ 使得它的预测 $\hat{y}$ 尽可能接近 Ground Truth y。

### 为什么我们需要 bias？

在**逻辑回归**或**线性回归**等任务中，偏置项 b 允许模型学习数据的真实分布，而不是强制其通过原点。

